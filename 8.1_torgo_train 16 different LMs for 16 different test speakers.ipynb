{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a11ac7-d254-4d40-b22f-fe3069c6deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%run 10_ngram_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdf65a7-30c0-44de-8817-f179048b1661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U datasets\n",
    "# !pip install fsspec==2023.9.2\n",
    "# pattern = \"no_keep\"\n",
    "# pattern = \"keep_all\"\n",
    "pattern = \"new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c513c7-12f9-4e83-a0e3-b619f10606db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers: F01, F03, F04, FC01, FC02, FC03, M01, M02, M03, M04, M05, MC01, MC02, MC03, MC04\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "data_df = pd.read_csv('torgo_new.csv')\n",
    "dataset_csv = load_dataset('csv', data_files='torgo_new.csv',cache_dir=None)\n",
    "\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "print(f'Speakers: {\", \".join(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10b482f-53e3-4b34-bc12-b082f36766e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 9749\n",
      "F03: 9551\n",
      "F04: 9551\n",
      "M01: 9519\n",
      "M02: 9508\n",
      "M03: 9493\n",
      "M04: 9559\n",
      "M05: 9594\n"
     ]
    }
   ],
   "source": [
    "atypical_speaker_texts = {}\n",
    "\n",
    "for speaker_id in atypical_speakers:\n",
    "    test_speaker=speaker_id\n",
    "    valid_speaker = 'F03' if test_speaker != 'F03' else 'F04'\n",
    "    train_speaker = [s for s in speakers if s not in [test_speaker, valid_speaker]]\n",
    "    \n",
    "    torgo_dataset = DatasetDict()\n",
    "    torgo_dataset['train'] = dataset_csv['train'].filter(lambda x: x in train_speaker, input_columns=['speaker_id'])\n",
    "    torgo_dataset['validation'] = dataset_csv['train'].filter(lambda x: x == valid_speaker, input_columns=['speaker_id'])\n",
    "    torgo_dataset['test'] = dataset_csv['train'].filter(lambda x: x == test_speaker, input_columns=['speaker_id'])\n",
    "\n",
    "    # print(torgo_dataset)\n",
    "    if pattern == \"no_keep\":\n",
    "        unique_texts = set(torgo_dataset['train'].unique(column='text')) | set(torgo_dataset['validation'].unique(column='text')) | set(torgo_dataset['test'].unique(column='text'))\n",
    "        unique_texts_count = {}\n",
    "        \n",
    "        for text in unique_texts:\n",
    "          unique_texts_count[text] = {'train_validation': 0, 'test': 0}\n",
    "        \n",
    "        for text in torgo_dataset['train']['text']:\n",
    "          unique_texts_count[text]['train_validation'] += 1\n",
    "        \n",
    "        for text in torgo_dataset['validation']['text']:\n",
    "          unique_texts_count[text]['train_validation'] += 1\n",
    "        \n",
    "        for text in torgo_dataset['test']['text']:\n",
    "          unique_texts_count[text]['test'] += 1\n",
    "        \n",
    "        texts_to_keep_in_train_validation = []\n",
    "        texts_to_keep_in_test = []\n",
    "        for text in unique_texts_count:\n",
    "          if unique_texts_count[text]['train_validation'] < text_count_threshold and unique_texts_count[text]['test'] > 0:\n",
    "            texts_to_keep_in_test.append(text)\n",
    "          else:\n",
    "            texts_to_keep_in_train_validation.append(text)\n",
    "        \n",
    "        original_data_count = {'train': len(torgo_dataset['train']), 'validation': len(torgo_dataset['validation']), 'test': len(torgo_dataset['test'])}\n",
    "        \n",
    "        # Update the three dataset splits\n",
    "        torgo_dataset['train'] = torgo_dataset['train'].filter(lambda x: x['text'] in texts_to_keep_in_train_validation)\n",
    "        torgo_dataset['validation'] = torgo_dataset['validation'].filter(lambda x: x['text'] in texts_to_keep_in_train_validation)\n",
    "        torgo_dataset['test'] = torgo_dataset['test'].filter(lambda x: x['text'] in texts_to_keep_in_test)\n",
    "        \n",
    "        print(f'Train:       {len(torgo_dataset[\"train\"])}/{original_data_count[\"train\"]} ({len(torgo_dataset[\"train\"]) * 100 // original_data_count[\"train\"]}%)')\n",
    "        print(f'Validation:  {len(torgo_dataset[\"validation\"])}/{original_data_count[\"validation\"]} ({len(torgo_dataset[\"validation\"]) * 100 // original_data_count[\"validation\"]}%)')\n",
    "        print(f'Test:        {len(torgo_dataset[\"test\"])}/{original_data_count[\"test\"]} ({len(torgo_dataset[\"test\"]) * 100 // original_data_count[\"test\"]}%)')\n",
    "        \n",
    "        print()\n",
    "        torgo_dataset\n",
    "    elif pattern == \"keep_all\":\n",
    "        pass\n",
    "    elif pattern == \"new\":\n",
    "            # Update the three dataset splits (if ['test_data'] == 1, keep in test, if ['test_data'] == 0, keep in train and validation)\n",
    "        torgo_dataset['train'] = torgo_dataset['train'].filter(\n",
    "            lambda x: x['test_data'] == 0)\n",
    "        torgo_dataset['validation'] = torgo_dataset['validation'].filter(\n",
    "            lambda x: x['test_data'] == 0)\n",
    "        torgo_dataset['test'] = torgo_dataset['test'].filter(\n",
    "            lambda x: x['test_data'] == 1)\n",
    "    \n",
    "            # Drop the 'test_data' column\n",
    "        torgo_dataset['train'] = torgo_dataset['train'].remove_columns([\n",
    "                                                                       'test_data'])\n",
    "        torgo_dataset['validation'] = torgo_dataset['validation'].remove_columns([\n",
    "                                                                                 'test_data'])\n",
    "        torgo_dataset['test'] = torgo_dataset['test'].remove_columns([\n",
    "                                                                     'test_data'])\n",
    "\n",
    "    \n",
    "    texts = torgo_dataset['train']['text']\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "        \n",
    "    atypical_speaker_texts[speaker_id] = cleaned_texts\n",
    "\n",
    "\n",
    "for speaker_id, texts in atypical_speaker_texts.items():\n",
    "    print(f\"{speaker_id}: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e6735f-0937-4eba-9d41-d433f0ec1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"torgo_lm_arpa_files\", exist_ok=True)\n",
    "\n",
    "for speaker_id, texts in atypical_speaker_texts.items():\n",
    "    with open(f\"torgo_lm_arpa_files/{speaker_id}_texts_{pattern}.txt\", \"w\") as file:\n",
    "        for text in texts:\n",
    "            file.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e1d06d-a8b7-4e79-a754-dee17cad7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/F01_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24694 types 1275\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15300 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1275 D1=0.808429 D2=1.24331 D3+=1.92209\n",
      "2 2576 D1=0.5 D2=1 D3+=1.5\n",
      "3 2134 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     67 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15300 2:41216 3:42680\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15300 2:41216 3:42680\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19799932 kB\tVmRSS:3836 kB\tRSSMax:4535788 kB\tuser:0.422314\tsys:2.96619\tCPU:3.38856\treal:3.39303\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/F03_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24173 types 1277\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15324 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1277 D1=0.811205 D2=1.20858 D3+=1.86431\n",
      "2 2584 D1=0.5 D2=1 D3+=1.5\n",
      "3 2142 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     68 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19799932 kB\tVmRSS:3836 kB\tRSSMax:4535796 kB\tuser:0.46445\tsys:3.15547\tCPU:3.61994\treal:3.62948\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/F04_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24173 types 1277\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15324 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1277 D1=0.811205 D2=1.20858 D3+=1.86431\n",
      "2 2584 D1=0.5 D2=1 D3+=1.5\n",
      "3 2142 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     68 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19799932 kB\tVmRSS:3840 kB\tRSSMax:4535796 kB\tuser:0.450937\tsys:3.17652\tCPU:3.62748\treal:3.63061\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M01_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24132 types 1274\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15288 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1274 D1=0.810769 D2=1.20901 D3+=1.86492\n",
      "2 2577 D1=0.5 D2=1 D3+=1.5\n",
      "3 2136 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     67 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15288 2:41232 3:42720\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15288 2:41232 3:42720\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19799932 kB\tVmRSS:3832 kB\tRSSMax:4535772 kB\tuser:0.453877\tsys:3.10332\tCPU:3.55723\treal:3.56332\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M02_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24111 types 1272\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15264 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1272 D1=0.811874 D2=1.18147 D3+=1.97031\n",
      "2 2572 D1=0.5 D2=1 D3+=1.5\n",
      "3 2134 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     67 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15264 2:41152 3:42680\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15264 2:41152 3:42680\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19799932 kB\tVmRSS:3836 kB\tRSSMax:4535768 kB\tuser:0.462768\tsys:3.1187\tCPU:3.58153\treal:3.5844\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M03_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24072 types 1277\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15324 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1277 D1=0.811205 D2=1.20858 D3+=1.86431\n",
      "2 2584 D1=0.5 D2=1 D3+=1.5\n",
      "3 2142 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     68 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19797876 kB\tVmRSS:3836 kB\tRSSMax:4535796 kB\tuser:0.474252\tsys:3.1081\tCPU:3.58241\treal:3.58586\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M04_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24220 types 1277\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15324 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1277 D1=0.812596 D2=1.20073 D3+=1.86237\n",
      "2 2582 D1=0.5 D2=1 D3+=1.5\n",
      "3 2140 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     68 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15324 2:41312 3:42800\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15324 2:41312 3:42800\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19799932 kB\tVmRSS:3832 kB\tRSSMax:4535764 kB\tuser:0.412917\tsys:3.14575\tCPU:3.55873\treal:3.56961\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M05_texts_new.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 24317 types 1277\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15324 2:6997409280 3:13120142336\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1277 D1=0.811205 D2=1.20858 D3+=1.86431\n",
      "2 2584 D1=0.5 D2=1 D3+=1.5\n",
      "3 2142 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 130 assuming -p 1.5\n",
      "probing 150 assuming -r models -p 1.5\n",
      "trie     68 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     66 assuming -a 22 array pointer compression\n",
      "trie     48 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15324 2:41344 3:42840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19799932 kB\tVmRSS:3844 kB\tRSSMax:4535768 kB\tuser:0.444328\tsys:3.12128\tCPU:3.56564\treal:3.57051\n"
     ]
    }
   ],
   "source": [
    "arpa_dir = \"torgo_lm_arpa_files\"\n",
    "os.makedirs(arpa_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(\"torgo_lm_arpa_files\"):\n",
    "    if file_name.endswith(f\"{pattern}.txt\"):\n",
    "        speaker_id = file_name.split(\"_\")[0]\n",
    "        arpa_file = f\"{arpa_dir}/{speaker_id}_3gram_{pattern}.arpa\"\n",
    "        txt_file = f\"torgo_lm_arpa_files/{file_name}\"\n",
    "        !kenlm/build/bin/lmplz -o 3 < \"{txt_file}\" > \"{arpa_file}\" -S 10% --discount_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3189db50-781e-46e3-be87-01294ff1d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atypical_speakers = ['F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05']\n",
    "\n",
    "for speaker in atypical_speakers:\n",
    "    arpa_file = f\"torgo_lm_arpa_files/{speaker}_3gram_{pattern}.arpa\"\n",
    "    output_file = f\"torgo_lm_arpa_files/{speaker}_unigram_{pattern}.txt\"\n",
    "\n",
    "    unigrams = []\n",
    "\n",
    "    with open(arpa_file, \"r\") as file:\n",
    "        start_extraction = False\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if start_extraction:\n",
    "                if not line:  \n",
    "                    break\n",
    "                parts = line.split(\"\\t\")\n",
    "                unigram = parts[1]  \n",
    "                unigrams.append(unigram)\n",
    "            elif line.startswith(\"\\\\1-grams:\"):\n",
    "                start_extraction = True\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for unigram in unigrams:\n",
    "            file.write(unigram + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f428d-6e04-48dc-9892-225df8f573a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
